import streamlit as st
from langchain.memory import ConversationBufferMemory
from langchain_groq import ChatGroq
from loader import *
import tempfile
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Tipos e modelos dispon√≠veis
tipoArquivoValido = ['PDF', 'CSV', 'WebSite']
modelosConfig = {
    'Groq': {
        'modelos': [
            'gpt-5-nano'
        ],
        'chat': ChatGroq,
    },
}

# Inicializa mem√≥ria no session_state
if 'memoria' not in st.session_state:
    st.session_state['memoria'] = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )

# Carrega o conte√∫do do arquivo
def carregaArquivo(tipoArquivo, arquivo):
    if tipoArquivo == 'WebSite':
        documento = carregaSite(arquivo)
    elif tipoArquivo == 'CSV':
        with tempfile.NamedTemporaryFile(suffix='.csv', delete=False) as temp:
            temp.write(arquivo.read())
            nomeTemp = temp.name
        documento = carregaCSV(nomeTemp)
    elif tipoArquivo == 'PDF':
        with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp:
            temp.write(arquivo.read())
            nomeTemp = temp.name
        documento = carregaPDF(nomeTemp)
    else:
        documento = "Tipo de arquivo n√£o suportado."
    return documento

# Carrega o modelo e prepara a cadeia de conversa√ß√£o
def carregaModelo(provedor, modelo, apiKey, tipoArquivo, arquivo):
    try:
        documento = carregaArquivo(tipoArquivo, arquivo)

        if "Just a moment..." in documento:
            st.warning("O conte√∫do do site n√£o p√¥de ser carregado corretamente. Tente novamente.")
            return

        system_message = f'''Voc√™ √© um assistente amig√°vel chamado VagnerGPT. Voc√™ possui acesso √†s seguintes informa√ß√µes vindas de um documento do tipo {tipoArquivo}:
####
{documento}
####
Utilize as informa√ß√µes fornecidas para basear as suas respostas.
Sempre que houver $ em sua sa√≠da, substitua por S.

Se a informa√ß√£o do documento for algo como "Just a moment...Enable JavaScript and cookies to continue", sugira ao usu√°rio carregar novamente o modelo.
'''

        template = ChatPromptTemplate.from_messages([
            ('system', system_message),
            ('placeholder', '{chat_history}'),
            ('user', '{input}')
        ])

        chat = modelosConfig[provedor]['chat'](model=modelo, api_key=apiKey)

        # Parser garante strings para o streaming no Streamlit
        chain = template | chat | StrOutputParser()
        st.session_state['chain'] = chain

    except Exception as e:
        st.error(f"Erro ao carregar modelo ou documento: {e}")

# P√°gina principal do chat
def pagina_chat():
    st.header('ü§ñ Welcome to VagnerGPT', divider=True)
    st.markdown("""
Welcome to your personal AI assistant!  
Here you can:
- üí¨ Ask questions based on PDF files, CSV data, or websites.  
- üìÑ View the extracted content from the uploaded file.  
- ‚öôÔ∏è Choose the language model to generate responses.
""")

    chain = st.session_state.get('chain')

    if chain is None:
        st.error('Carregue o modelo antes de conversar.')
        st.stop()

    memoria = st.session_state['memoria']

    # Exibe o hist√≥rico (mapeia 'human'/'ai' -> 'user'/'assistant')
    for mensagem in memoria.buffer_as_messages:
        role = 'user' if mensagem.type in ('human', 'user') else 'assistant'
        chat_msg = st.chat_message(role)
        chat_msg.markdown(mensagem.content)

    input_usuario = st.chat_input('Talk to me')

    if input_usuario:
        st.chat_message('user').markdown(input_usuario)

        # Executa o modelo e escreve a resposta em streaming
        resposta_stream = chain.stream({
            'input': input_usuario,
            'chat_history': memoria.buffer_as_messages
        })
        resposta_chat = st.chat_message('assistant')
        resposta_completa = resposta_chat.write_stream(resposta_stream)

        # Salva no hist√≥rico da mem√≥ria
        memoria.chat_memory.add_user_message(input_usuario)
        memoria.chat_memory.add_ai_message(resposta_completa)

        st.session_state['memoria'] = memoria

# Sidebar de configura√ß√£o
def sidebar():
    tabs = st.tabs(['üìÅ File Upload', '‚öôÔ∏è Model Selection'])

    with tabs[0]:
        tipoArquivo = st.selectbox('Select source type', tipoArquivoValido)
        if tipoArquivo == 'WebSite':
            arquivo = st.text_input('Enter website URL')
        elif tipoArquivo == 'CSV':
            arquivo = st.file_uploader('Upload CSV file', type=['.csv'])
        elif tipoArquivo == 'PDF':
            arquivo = st.file_uploader('Upload PDF file', type=['.pdf'])
        else:
            arquivo = None

        provedor = 'Groq'
        modelo = 'llama-3.3-70b-versatile'
        apiKey = st.secrets["GROQ_API_KEY"]


    if st.button('üöÄ Load file'):
        if not arquivo:
            st.warning("Voc√™ precisa fornecer um arquivo ou URL.")
        else:
            with st.spinner("Carregando modelo e documento..."):
                carregaModelo(provedor, modelo, apiKey, tipoArquivo, arquivo)

# Fun√ß√£o principal
def main():
    with st.sidebar:
        sidebar()
    pagina_chat()

if __name__ == '__main__':
    main()
